{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Wrangling\n",
    "### Luiz Schiller\n",
    "### Map Area: Rio de Janeiro, Brazil\n",
    "- https://mapzen.com/data/metro-extracts/metro/rio-de-janeiro_brazil/\n",
    "\n",
    "This area contains three cities that had a great part in my history. I lived about one third of my life on each: Petrópolis (where I was born), Niterói and Rio de Janeiro. That said, I would like to explore this extract a little bit and see what interesting data I can find.\n",
    "\n",
    "## Problems Encountered in the Map\n",
    "\n",
    "After the initial cleaning on the data from the downloaded xml file, it was imported into mongodb using the following command:\n",
    "```\n",
    "mongoimport --db osm --collection rio --file rio-de-janeiro_brazil.osm.json\n",
    "```\n",
    "\n",
    "The elements were structured like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "\"id\": \"2406124091\",\n",
    "\"type\": \"node\",\n",
    "\"created\": {\n",
    "          \"version\":\"2\",\n",
    "          \"changeset\":\"17206049\",\n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",\n",
    "          \"user\":\"linuxUser16\",\n",
    "          \"uid\":\"1219059\"\n",
    "        },\n",
    "\"pos\": [41.9757030, -87.6921867],\n",
    "\"address\": {\n",
    "          \"housenumber\": \"5157\",\n",
    "          \"postcode\": \"24230-062\",\n",
    "          \"street\": \"Rua Moreira César\"\n",
    "        },\n",
    "\"amenity\": \"restaurant\",\n",
    "\"cuisine\": \"mexican\",\n",
    "\"name\": \"La Cabana De Don Luis\",\n",
    "\"phone\": \"+55-21-95757782\"\n",
    "}\n",
    "```\n",
    "\n",
    "Analyzing a sample of the data, some problems showed up:\n",
    "\n",
    "- Tags with k=\"type\" overriding the element's 'type' field;\n",
    "- String 'bicycle_parking' capacities instead of numbers;\n",
    "- Abbreviated street types in 'address.street' tag;\n",
    "- Many different formats in 'phone' field;\n",
    "- pprint.pprint method not printing Unicode characters properly.\n",
    "\n",
    "### Tags with k=\"type\" overriding the element's 'type' field\n",
    "Second level 'k' tags with the value 'type' were overriding the element's 'type' field, which should equal 'node' or 'way' only. These tags were mapped to the element with the 'type_tag' key before being imported to mongodb.\n",
    "\n",
    "### String 'bicycle_parking' capacities instead of numbers\n",
    "Nodes representing bicycle parkings had their capacity fields as strings, which did not allow numeric operations I was willing to make with them. All of them represented numbers, except for one '§0' value. To solve this, I iterated over the xml file, updating the values with the parsed integer values. Whenever the parsing failed, the 'capacity' field was removed. The code used for the removal is shown below:\n",
    "\n",
    "```python\n",
    "def handle_bicycle_parking_capacity(node):\n",
    "    if ('amenity' in node) and (node['amenity'] == 'bicycle_parking'):\n",
    "        if 'capacity' in node:\n",
    "            try:\n",
    "                node['capacity'] = int(node['capacity'])\n",
    "            except ValueError:\n",
    "                node.pop('capacity')\n",
    "```\n",
    "\n",
    "### Abbreviated street types in 'address.street' tag\n",
    "There were several street names with it's type abbreviated, for example:\n",
    "```\n",
    "Estr. da Paciência\n",
    "Av Castelo Branco\n",
    "R. Miguel Gustavo\n",
    "```\n",
    "It is worth noting that in Portuguese the street types appear at the beginning of a street name, in contrast with English, where it appears at the end.\n",
    "To deal with this a mapping was created to convert abbreviations to complete street types:\n",
    "```python\n",
    "mapping = { \"Av\": \"Avenida\",\n",
    "            \"Av.\": \"Avenida\",\n",
    "            \"Est.\": \"Estrada\",\n",
    "            \"Estr.\": \"Estrada\",\n",
    "            \"estrada\": \"Estrada\",\n",
    "            \"Pca\": u\"Praça\",\n",
    "            \"Praca\": u\"Praça\",\n",
    "            u\"Pça\": u\"Praça\",\n",
    "            u\"Pça.\": u\"Praça\",\n",
    "            \"R.\": \"Rua\",\n",
    "            \"RUA\": \"Rua\",\n",
    "            \"rua\": \"Rua\",\n",
    "            \"Ruas\": \"Rua\",\n",
    "            \"Rue\": \"Rua\",\n",
    "            \"Rod.\": \"Rodovia\",\n",
    "            \"Trav\": \"Travessa\" }\n",
    "```\n",
    "After the update, the abbreviation problem was solved for almost all cases, excluding only stranger ones probably caused by human erroneous inputs.\n",
    "\n",
    "### Many different formats in 'phone' field\n",
    "The 'phone' field of most elements was filled with various different formats of phone number, and many times more than one phone number was inserted in the same field.\n",
    "To organize this data I defined a standard pattern for the phone values, and audited the file classifying the values into four groups: ok, wrong_separators, missing_area_code and other. The groups were defined by regular expressions as follows:\n",
    "#### ok\n",
    "```\n",
    "# +55 99 99999999\n",
    "phone_ok_re = re.compile(r'^\\+55\\s\\d{2}\\s\\d{8,9}$')\n",
    "# 0800 999 9999\n",
    "phone_0800_ok_re = re.compile(r'^0800\\s\\d{3}\\s\\d{4}$')\n",
    "```\n",
    "#### wrong_separators\n",
    "```\n",
    "# 55-99-9-99999999\n",
    "wrong_separators_re = re.compile(r'^\\D*55\\D*\\d{2}\\D*(\\d\\D?)?\\d{4}\\D?\\d{4}$')\n",
    "# +55-99-0800-999-9999\n",
    "wrong_separators_0800_re = re.compile(r'^\\D*(55)?\\D*(\\d{2})?\\D*0800\\D?\\d{3}\\D?\\d\\D?\\d{3}$')\n",
    "```\n",
    "#### missing_area_code\n",
    "```\n",
    "# missing +55 (Rio area codes start with 2)\n",
    "missing_ddi_re = re.compile(r'^\\D*2\\d\\D*(\\d\\D?)?\\d{4}\\D?\\d{4}$')\n",
    "# missing +55 2X\n",
    "missing_ddd_re = re.compile(r'^(\\d\\D?)?\\d{4}\\D?\\d{4}$')\n",
    "```\n",
    "#### other\n",
    "    The remaining values.\n",
    "    \n",
    "Before the update of the values, which consisted in turning the phone values into a list of strings, removing non-alphanumeric values, including area codes and including spaces only when it was appropriated, the classification was like this:\n",
    "```json\n",
    "{\n",
    "  \"missing_area_code\": 72, \n",
    "  \"wrong_separators\": 2055, \n",
    "  \"other\": 41, \n",
    "  \"ok\": 151\n",
    "}\n",
    "```\n",
    "and after the update it turned out like this:\n",
    "```json\n",
    "{\n",
    "  \"missing_area_code\": 18, \n",
    "  \"wrong_separators\": 0, \n",
    "  \"other\": 41, \n",
    "  \"ok\": 2260\n",
    "}\n",
    "```\n",
    "With an upgrade from 6.5% to 97.5% of 'ok' values, I was content with the phones cleaning for this wrangling exercise.\n",
    "\n",
    "\n",
    "### pprint.pprint method not printing Unicode characters properly\n",
    "This problem is not related to the data itself, but it was harming the wrangling process.\n",
    "When printing the results of some queries with the pprint.pprint method, characters out of the ascii table showed as their Unicode representation, making it hard to read.\n",
    "To solve this I had to instantiate my own printer, witch encoded unicode objects to utf-8, making it possible to read. Check the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "class MyPrettyPrinter(pprint.PrettyPrinter):\n",
    "    def format(self, object, context, maxlevels, level):\n",
    "        if isinstance(object, unicode):\n",
    "            return (object.encode('utf8'), True, False)\n",
    "        return pprint.PrettyPrinter.format(self, object, context, maxlevels, level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them. Some queries make use of the 'aggregate' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def get_db(db_name):\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.rio.aggregate(pipeline)]\n",
    "\n",
    "db = get_db('osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Sizes\n",
    "\n",
    "```\n",
    "rio-de-janeiro_brazil.osm ........... 323 MB\n",
    "rio-de-janeiro_brazil.osm.json ...... 369 MB\n",
    "```\n",
    "### Elements Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737174"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.rio.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1550716"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node count\n",
    "db.rio.find({'type': 'node'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186458"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# way count\n",
    "db.rio.find({'type': 'way'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Distinct Users\n",
    "This query uses the following 'aggregate' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: Distinct users:, count: 1239}]\n"
     ]
    }
   ],
   "source": [
    "distinct_users = [\n",
    "    {'$group': {'_id': '$created.user'}},\n",
    "    {'$group': {'_id': 'Distinct users:', 'count': {'$sum': 1}}}]\n",
    "result = aggregate(db, distinct_users)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Contributing Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: Alexandrecw, count: 374621},\n",
      " {_id: ThiagoPv, count: 186562},\n",
      " {_id: smaprs_import, count: 185690},\n",
      " {_id: AlNo, count: 169678},\n",
      " {_id: Import Rio, count: 85129},\n",
      " {_id: Geaquinto, count: 69987},\n",
      " {_id: Nighto, count: 63148},\n",
      " {_id: Thundercel, count: 55004},\n",
      " {_id: Márcio Vínícius Pinheiro, count: 35985},\n",
      " {_id: smaprs, count: 31507}]\n"
     ]
    }
   ],
   "source": [
    "top_10_users = [\n",
    "    {'$group': {'_id': '$created.user', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}]\n",
    "result = aggregate(db, top_10_users)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Appearing Only Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: 1, num_users: 274}]\n"
     ]
    }
   ],
   "source": [
    "users_appearing_once = [\n",
    "    {'$group': {'_id': '$created.user', 'count': {'$sum':1}}},\n",
    "    {'$group': {'_id': '$count', 'num_users': {'$sum':1}}},\n",
    "    {'$sort': {'_id': 1}},\n",
    "    {'$limit': 1}]\n",
    "result = aggregate(db, users_appearing_once)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Ideas\n",
    "\n",
    "### City validation based on postcodes\n",
    "The city and postcode values could be crosschecked when inputing a new address. Most countries have public APIs to retrieve addresses from postcodes, so it could be done, with the help of contributors around the world.\n",
    "\n",
    "### Phone format validator\n",
    "The Open Street Map input tool could have a phone format validator, varying from country to country, to avoid such a mess on the phones format 😉. It could also separate multiple phones with a standard separator, since it was one of the most difficult steps of the phone values wrangling.\n",
    "The fact that each country has a different standard format makes it difficult to implement this, but with the help of the open software community around Open Street Map it could be done.\n",
    "\n",
    "### Variety.js\n",
    "The open-source tool Variety (https://github.com/variety/variety) allows the user get a sense of how the data is structured in a MongoDB schema. It does so by showing the number of occurences for each key on documents returned by a query.\n",
    "It is a useful ally when analysing datasets like Open Street Map, which does not define an allowed key set.\n",
    "\n",
    "### Most Common Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: school, count: 1818},\n",
      " {_id: bicycle_parking, count: 1409},\n",
      " {_id: restaurant, count: 1080},\n",
      " {_id: parking, count: 976},\n",
      " {_id: fast_food, count: 890},\n",
      " {_id: fuel, count: 678},\n",
      " {_id: place_of_worship, count: 562},\n",
      " {_id: bank, count: 534},\n",
      " {_id: pub, count: 400},\n",
      " {_id: pharmacy, count: 368}]\n"
     ]
    }
   ],
   "source": [
    "most_common_amenities = [\n",
    "    {'$match': {'amenity': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$amenity', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}]\n",
    "result = aggregate(db, most_common_amenities)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on Bike Parking Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: Bike parking stats:,\n",
      "  avg: 11.487840825350037,\n",
      "  count: 1357,\n",
      "  max: 700,\n",
      "  min: 1}]\n"
     ]
    }
   ],
   "source": [
    "bike_parkings_capacity = [\n",
    "    {'$match': {'amenity': 'bicycle_parking', 'capacity': {'$exists': 1}}},\n",
    "    {'$group': {\n",
    "            '_id': 'Bike parking stats:',\n",
    "            'count': {'$sum': 1},\n",
    "            'max': {'$max': '$capacity'},\n",
    "            'min': {'$min': '$capacity'},\n",
    "            'avg': {'$avg': '$capacity'}}}]\n",
    "result = aggregate(db, bike_parkings_capacity)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Most Common Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: pizza, count: 88},\n",
      " {_id: regional, count: 83},\n",
      " {_id: japanese, count: 38},\n",
      " {_id: italian, count: 38},\n",
      " {_id: steak_house, count: 20},\n",
      " {_id: barbecue, count: 18},\n",
      " {_id: brazilian, count: 16},\n",
      " {_id: international, count: 12},\n",
      " {_id: seafood, count: 8},\n",
      " {_id: chinese, count: 8}]\n"
     ]
    }
   ],
   "source": [
    "top_10_cuisines = [\n",
    "    {'$match': {'amenity': 'restaurant', 'cuisine': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$cuisine', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}]\n",
    "result = aggregate(db, top_10_cuisines)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Most Common Religions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: christian, count: 495},\n",
      " {_id: spiritualist, count: 7},\n",
      " {_id: jewish, count: 6},\n",
      " {_id: buddhist, count: 3},\n",
      " {_id: religion_of_humanity, count: 1},\n",
      " {_id: umbanda, count: 1},\n",
      " {_id: macumba, count: 1},\n",
      " {_id: muslim, count: 1},\n",
      " {_id: seicho_no_ie, count: 1}]\n"
     ]
    }
   ],
   "source": [
    "most_common_religions = [\n",
    "    {'$match': {'amenity': 'place_of_worship', 'religion': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$religion', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}]\n",
    "result = aggregate(db, most_common_religions)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority is christian. Among them, which are the most common denominations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{_id: catholic, count: 157},\n",
      " {_id: baptist, count: 33},\n",
      " {_id: roman_catholic, count: 31},\n",
      " {_id: evangelical, count: 27},\n",
      " {_id: spiritist, count: 20},\n",
      " {_id: pentecostal, count: 19},\n",
      " {_id: protestant, count: 14},\n",
      " {_id: methodist, count: 10},\n",
      " {_id: presbyterian, count: 3},\n",
      " {_id: assemblies_of_god, count: 2}]\n"
     ]
    }
   ],
   "source": [
    "christian_denominations = [\n",
    "    {'$match': {'amenity': 'place_of_worship', 'religion': 'christian', 'denomination': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$denomination', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}]\n",
    "result = aggregate(db, christian_denominations)\n",
    "MyPrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast-food Sites Near the Sugar Loaf\n",
    "Consider you are visiting the Sugar Loaf in Rio and suddenly you are starving! Where to go?\n",
    "MongoDB Geospacial Index to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{cuisine: corn, name: Tino},\n",
      " {cuisine: sandwich, name: Max},\n",
      " {cuisine: popcorn, name: França}]\n"
     ]
    }
   ],
   "source": [
    "from pymongo import GEO2D\n",
    "\n",
    "db.rio.create_index([('pos', GEO2D)])\n",
    "\n",
    "sugar_loaf = db.rio.find_one({'name': 'Pão de Açúcar', 'tourism': 'attraction'})\n",
    "\n",
    "result = db.rio.find(\n",
    "    {'pos': {'$near': sugar_loaf['pos']}, 'amenity': 'fast_food'},\n",
    "    {'_id': 0, 'name': 1, 'cuisine': 1}).skip(1).limit(3)\n",
    "\n",
    "MyPrettyPrinter().pprint([item for item in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Luckily there are Tino's corn, Max's sandwich and França's popcorn to satisfy your hunger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Data inserted by humans is almost certain to show inconsistencies. And even though a big part of it is inserted by bots, different bots may insert data using different patterns, and the inconsistency remains. On the other hand, this freedom on the data input grants a lot of flexibility to users, and because of that, the representation of the map may be even more faithful to the real world than if there were key constraints or limitations.\n",
    "\n",
    "Anyway, for the purposes of this wrangling exercise the data has been well cleaned.\n",
    "\n",
    "\n",
    "\n",
    "### References:\n",
    "#### pprint Unicode\n",
    "- http://stackoverflow.com/questions/10883399/unable-to-encode-decode-pprint-output\n",
    "\n",
    "#### MongoDB Geospacial Index\n",
    "- https://docs.mongodb.com/v3.2/tutorial/build-a-2d-index/\n",
    "- https://docs.mongodb.com/v3.2/tutorial/query-a-2d-index/\n",
    "- http://api.mongodb.com/python/current/api/pymongo/collection.html?_ga=1.25837502.2095208423.1476211996#pymongo.collection.Collection.create_index\n",
    "\n",
    "#### Variety Open Source Tool\n",
    "- https://github.com/variety/variety"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
